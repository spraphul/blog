---
title: Reinforcement Learning &#58; Creating a Custom Environment
---

![RL21](https://www.learndatasci.com/documents/14/Reinforcement-Learning-Animation.gif)

In the previous blog, I had gone through the training of an agent for a mountain car environment provided by gym library. But what if we need the training for an environment which is not in gym.


Sometimes we will need to create our own environments. This blog is all about creating a custom environment from scratch.

The environment which we will be creating here will be a grid containing two policemen, one thief and one bags of golds. The goal of the thief is to get the bag without being caught by the policemen.

All the parameters for training the model will be similar to that of the earlier post except some codes for the custom environment.

Now, let us write a python class for our environment which we will call a grid. 


<script src="https://gist.github.com/spraphul/091355c044e5934ad76cc7036f8868e5.js"></script>


Now, that we have defined our grid, we will assign one grid to each of the policemen, thief and the bag of gold. One more thing to note is that we will give a negative penalty to the thief when it touches the police and some positive reward when it touches the gold bag. In this case, our observation space will be the difference of coordinates of the thief from each of the policemen and the gold bag. We will also assign some colours for the police, thief and gold bag.

Let us write the code for training our game.


<script src="https://gist.github.com/spraphul/b1e7a87206e79f262f19e60d6c919886.js"></script>


I will quickly go through the code once again. Initially, I have defined the different penalties, number of episodes, learning rate and the discount factor. Then I created a q-table in the form of a dictionary with random values in it for all the 8 possible actions. Then I have written the code for training the model in which for each episode, the thief takes a maximum of 200 steps and stops before if it reaches the gold bag. After every 1000 episodes, I am showing the trace of the path of the thief using cv2 and PIL image library. One important thing to note was that what happens if the thief reaches one of the policemen or the gold bag. It may cause a pause and that's why we are breaking the loop in case those things occur.

Note: The environment we just created requires a q-table which will be huge in memory and thus it may take too much time to create the q-table itself. The recommended solution is to reduce the number of policemen or the grid size so that you can visualise it in a usual computer. If you already have the q-table, it is recommended to save it as a binary pickle file and load while training the model.


I have trained the model for 1 policeman and 5X5 grid and the results per 100 episodes are as shown in the video below:

![RL22](https://1.bp.blogspot.com/-PCnMyBrDzYE/Xd_J5UM9VrI/AAAAAAAAQEY/7Genwz99s9Aa2jVJ2mdGbvSakaV051MtACLcBGAsYHQ/s1600/customrl.gif)

So in this video, the blue colour box is the thief, green is the gold and red is the police. Whenever you see a combination of red and blue but no green, this means the thief has reached the gold bag.

Hope the second blog in the series was fun too. Keep Learning, Keep Sharing ðŸ˜Š
