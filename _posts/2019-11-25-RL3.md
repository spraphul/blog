---
title: Reinforcement Learning &#58; Deep Q Networks
---

In the previous two blogs, I went through the basics of Q-learning, building a simple RL Agent using q-table and creating a custom 
environment for our own use case. But as we keep making the environment bigger and complex, the q-table becomes bigger in size 
and this causes a scaling problem. I studied about Deep Q-learning which solves this problem and I will be going through the topic in detail in this blog.


![RL31](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2019/04/Screenshot-2019-04-16-at-5.46.01-PM.png)


We will be making use of the fact that neural networks are very good function approximators. You just need to have some weight values and can approximate various complex functions. The q-tables discussed previously, were used to store some values for all possible actions given a particular state. Based on those values, we used to decide which action to take further. Now instead of storing those values, we can train a neural network which predicts the action to take in the future, given the current state.
This can prove to be a relief for the scaling problem. We will be creating a deep Q-Agent using CNNs and will feed input as images of the states in it.

The Q-Agent class will contain two models, one for training and one for prediction. We will keep track of a limited number of past steps of the agent as we need to feed the data in batches(well, batch_size can be one too but it's always better to use mini-batches). 

**Given below is the code for the Q-Agent class.**

```python
class DAgent():
  def __init__(self):
    self.train_model = self.create_graph()
    self.predict_model = self.create_graph()
    
    set.predict_model.set_weights(self.train_model.get_weights())
    
    self.train_data = deque(maxlen=20000)
    
    self.set_predict_model_count = 0
    
  def create_graph(self):
    model = Sequential()
    model.add(Conv2D(100,(3,3)), input_shape=env.observation_space_shape)
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.2))

    model.add(Conv2D(50, (2, 2)))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.2))

    model.add(Flatten())  
    model.add(Dense(128))

    model.add(Dense(env.action_space_n, activation='linear'))  
    model.compile(loss="mse", optimizer=Adam(lr=0.001), metrics=['accuracy'])
    return model
  
  def update_train_data(self, transition):
    self.train_data.append(transition)
    
  
  def train(self,terminal_state, step):
    if(len(self.train_data)<min_train_data):
      return
    
    batch = random.sample(self.train_data, batch_size)
    current_states = np.array([transition[0] for transition in batch])/255
    current_q_values = self.train_model.predict(current_states)
    new_states = np.array([transition[3] for transition in batch])/255
    new_q_values = self.predict_model.predict(new_states)
    
    X = []; Y = []
    
    for index, (current_state, action, reward, new_state, done) in enumerate(batch):
      if not done:
          max_future_q_value = np.max(future_q_values[index])
          new_q = reward + DISCOUNT * max_future_q_value
      else:
          new_q = reward

      # Update Q value for given state
      current_q_values = current_qs_list[index]
      current_q_values[action] = new_q

      # And append to our training data
      X.append(current_state)
      Y.append(current_q_values)
      
      
    self.train_model.fit(np.array(X)/255, np.array(Y), batch_size=batch_size, verbose=0)
    if(terminal_state):
      self.set_predict_model_count +=1
    
    
    if(self.set_predict_model_count%5==0):
      self.predict_model.set_weights(self.train_model.get_weights())
      save_model(predict_model, 'deepQLearning.h5')
     
 def get_q_value(self, state):
    return self.train_model.predict(np.array(state).reshape(-1, *state.shape)/255)[0]

```


<script src="https://gist.github.com/spraphul/3ccd27a56bca2d426e5cb01c04e5c762.js"></script> <br/>


**We will be using the same Grid which we used in the previous blog for creating our custom environment which is as follows:**


<script src="https://gist.github.com/spraphul/091355c044e5934ad76cc7036f8868e5.js"></script>


Since we are using deep neural network here and we need to store various states for the training, we will be creating a Grid_env class very similar to that of a gym environment which returns the new state, reward and the done value, so that it becomes more structured and easy to handle. This class will have reset, step and render functions similar to that of gym and an extra return_img function for the data of the deep-learning model we created above.

**Let us create an environment class:**

<script src="https://gist.github.com/spraphul/8fea9ad8cc2883601b6eb275e3af324c.js"></script>


**Now comes the training part. We will train the agent for 50000 episodes and render the environment every 1000 episodes. The code for training will look as follows:**

<script src="https://gist.github.com/spraphul/3cd4070b86c37f2ac81e3c7c33394dee.js"></script>


I have also created a list which stores the reward for each episode which when plotted, ideally should be an increasing graph. I have enabled the player only for the movement but we can play around with it and enable enemy and food also. We can also increase  the number of food and the thiefs to make the game more challenging. It would be more cool, if we change the entire environment and create a new game itself. Meanwhile, we have successfully created a deep QAgent for cool RL games. In the upcoming blogs, We will be studying and implementing some interesting RL Algorithms proposed and used by DeepMind and OpenAI.
A huge credit for the past blogs goes to [sentdex](https://www.youtube.com/user/sentdex) for creating such nice tutorials.

**Keep Learning, Keep Sharing ðŸ˜Š**
