---
title: Reinforcement Learning &#58; Deep Q Networks
---

In the previous two blogs, I went through the basics of Q-learning, building a simple RL Agent using q-table and creating a custom 
environment for our own use case. But as we keep making the environment bigger and complex, the q-table becomes bigger in size 
and this causes a scaling problem. I studied about Deep Q-learning which solves this problem and I will be going through the topic in detail in this blog.


![RL31](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2019/04/Screenshot-2019-04-16-at-5.46.01-PM.png)


We will be making use of the fact that neural networks are very good function approximators. You just need to have some weight values and can approximate various complex functions. The q-tables discussed previously, were used to store some values for all possible actions given a particular state. Based on those values, we used to decide which action to take further. Now instead of storing those values, we can train a neural network which predicts the action to take in the future, given the current state.
This can prove to be a relief for the scaling problem. We will be creating a deep Q-Agent using CNNs and will feed input as images of the states in it.

The Q-Agent class will contain two models, one for training and one for prediction. We will keep track of a limited number of past steps of the agent as we need to feed the data in batches(well, batch_size can be one too but it's always better to use mini-batches). 

**Given below is the code for the Q-Agent class.**


<script src="https://gist.github.com/spraphul/3ccd27a56bca2d426e5cb01c04e5c762.js"></script> <br/>


**We will be using the same Grid which we used in the previous blog for creating our custom environment which is as follows:**


<script src="https://gist.github.com/spraphul/091355c044e5934ad76cc7036f8868e5.js"></script>


Since we are using deep neural network here and we need to store various states for the training, we will be creating a Grid_env class very similar to that of a gym environment which returns the new state, reward and the done value, so that it becomes more structured.



Now comes the training part.





