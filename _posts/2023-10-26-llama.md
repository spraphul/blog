---
title: Build your own 🦙chat
---
Ever wanted to create your very own chatbot, just like the ones you talk to on the internet? Well, you're in for a treat! In this blog, I'll guide you through the process using Llama-2 LLM.
But wait, there's more. I'll also help you design a cool and easy-to-use chatbot interface. We'll do this using Streamlit, which is a super user-friendly tool for making your chatbot look great.
Now, here's a key point: you'll need a computer with at least 16 GB of memory to make this work. Don't worry, though. I'll explain it all step by step, and the results will be fantastic!

## Serving the Llama-2 Model for Live Chat Generation
Before we can dive into building a chatbot interface, we need to get our Llama-2 model up and running. We want it to generate responses in real-time, just like the impressive ChatGPT. We will be building a simple FastAPI app which exposes an endpoint
for streamed text generation. Now, there can be cases when you don't usually have a powerful gpu machine like A-100s with 40/80 GB of memory. Thus, we will be loading the model in 4-bits. I will also show how to generate stream text using a LLM just 
like ChatGPT for better user experience. Now, lets get into the code part:


```python
import json
import torch
from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from transformers import AutoModelForCausalLM, AutoTokenizer
from pydantic import BaseModel, Field
from typing import Optional

checkpoint = "Llama2-7b-chat-hf/"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForCausalLM.from_pretrained(checkpoint,
                                    load_in_4bit=True,
                                    torch_dtype=torch.bfloat16,
                                    device_map="auto")

app = FastAPI()

class InputData(BaseModel):
    query: str
    tokens: Optional[int] = 400

def starts_with_space(token_id):
    token = tokenizer.convert_ids_to_tokens(token_id)
    return token.startswith('▁')

def live_answer(ask):
    input = tokenizer.encode(ask, return_tensors='pt')
    input_len = input.shape[1]
    generate = True
    output_length = 0
    while generate:
        output = model.generate(input, max_new_tokens=1,
                            do_sample=True, top_k=10,
                            temperature=0.5)
        current_token_id = output[0][-1]
        if current_token_id == tokenizer.eos_token_id:
            generate = False
            break

        current_token = tokenizer.decode(
            current_token_id, skip_special_tokens=True
        )
        if starts_with_space(current_token_id.item()) and output_length >= 1:
            current_token = ' ' + current_token
        yield current_token
        output_length += 1

        input = output

@app.post("/llama-7b/live_generate/")
async def live_generate(data: InputData):
    ask = PROMPT_TEMPLATE.format(instruction=data.query)
    text_data = live_answer(ask)
    return StreamingResponse(text_data, media_type="text/plain")
```


