---
title: Preference Training for LLMs in a Nutshell
---

![rlhf](https://github.com/spraphul/blog/blob/gh-pages/_posts/rlhf.png?raw=true)

Large Language Models (LLMs) harness the power of unsupervised learning(self-supervised to be precise) on extensive datasets, showcasing remarkable natural language processing capabilities. However, these models exhibit inherent limitations when not trained using Reinforcement Learning with Human Feedback (RLHF). One significant drawback lies in their lack of specificity and control. Although LLMs can generate contextually relevant outputs, they might fail to precisely adhere to user instructions, resulting in technically correct yet divergent responses. The absence of a mechanism for real-time adaptation to user feedback during inference further adds to this limitation, hindering the models from refining their outputs based on corrective input.

Ethical concerns and biases also affect LLMs not trained with RLHF. These models inadvertently showcase biases present in the training data, leading to outputs that may be skewed or unfair. Additionally, their vulnerability to generating incorrect or misleading information poses challenges in contexts where accuracy is critical. Despite their contextual understanding, LLMs may struggle with commonsense reasoning and language variations, impacting their ability to provide accurate and nuanced responses. Integrating RLHF into the training process addresses these flaws by enabling the model to learn from human feedback, rectify errors, and adapt to user preferences. This iterative learning approach enhances the model's overall performance, mitigating biases, improving accuracy, and enabling better alignment with user expectations. 

In this blog, we'll explore different ways to make Large Language Models (LLMs) better understand and respond to users' preferences. Firstly, we'll talk about the most talked method called Reinforcement Learning with Human Feedback (RLHF), using a reinforcement learning optimization called Proximal Policy Optimization (PPO). This helps the model learn from user feedback and improve its responses over time.

Next, we'll look at some modifications done by Meta for training their LLama-2 models. They use two reward models instead of one, making the model more sensitive to users' preferences. They also use rejection sampling to enable the model generate responses with maximum possible rewards. Lastly, we'll discuss a few alternatives to RLHF, like Direct Preference Optimization(DPO), and Reinforced Self-Training which mitigate the use of reward models and directly optimizing the policy(LLM) using the preference data.

## Reinforcement Learning with Human Feedback
RLHF was introduced by OpenAI in the paper titled "Training language models to follow instructions with human feedback". The authors describe their efforts in advancing the alignment of language models by training them to align with user intentions. They emphasize the importance of aligning language models with both explicit and implicit user intentions, such as following instructions, staying truthful, and avoiding biases or toxicity. The primary focus is on fine-tuning approaches, specifically using Reinforcement Learning from Human Feedback (RLHF) with Proximal Policy Optimization (PPO) to align the GPT-3 model to a broad range of written instructions.

The process involves hiring contractors to label data, collecting datasets of human-written demonstrations and comparisons, training reward models, and fine-tuning the model to maximize the reward using PPO. The resulting model, called InstructGPT, aligns with the preferences of a specific group of people, as opposed to a broader notion of "human values." Evaluation involves labeler ratings, automatic evaluations on NLP datasets, and comparisons between InstructGPT and GPT-3. Notable findings include labeler preferences for InstructGPT outputs, improvements in truthfulness, small gains in toxicity reduction, and generalization to preferences of held-out labelers. While exact training details for the best performing models like ChatGPT and GPT-4 haven't been released by OpenAI, the models' ability to adapt excellently to specific user preferences hints at RLHF as the core methodology behind the behaviour. 

A model having a behavior like ChatGPT is usually trained in three steps, unsupervised continual pretraining on a very large corpus followed by supervised finetuning on a high quality instruction-response pair dataset. At Last, RLHF comes into picture which is implemented as follows:

- Data Collection

Collect a dataset of human-labeled comparisons, where human annotators rank different model-generated outputs based on their quality or preference.

- Reward Model (RM) Training

Create a reward model by training it on the comparison data. The reward model predicts the preferences of human annotators by learning which model-generated outputs they prefer in different scenarios. The reward model in this case is also an LLM with the last token predictor replaced with a softmax layer which predicts a scaler reward for an instruction-response pair.

