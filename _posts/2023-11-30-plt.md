---
title: Human Preference Training for LLMs in a Nutshell
---

![rlhf](https://github.com/spraphul/blog/blob/gh-pages/_posts/rlhf.png?raw=true)

Large Language Models (LLMs) harness the power of unsupervised learning on extensive datasets, showcasing remarkable natural language processing capabilities. However, these models exhibit inherent limitations when not trained using Reinforcement Learning with Human Feedback (RLHF). One significant drawback lies in their lack of specificity and control. Although LLMs can generate contextually relevant outputs, they might fail to precisely adhere to user instructions, resulting in technically correct yet divergent responses. The absence of a mechanism for real-time adaptation to user feedback during inference further adds to this limitation, hindering the models from refining their outputs based on corrective input.

Ethical concerns and biases also affect LLMs not trained with RLHF. These models inadvertently showcase biases present in the training data, leading to outputs that may be skewed or unfair. Additionally, their vulnerability to generating incorrect or misleading information poses challenges in contexts where accuracy is critical. Despite their contextual understanding, LLMs may struggle with commonsense reasoning and language variations, impacting their ability to provide accurate and nuanced responses. Integrating RLHF into the training process addresses these flaws by enabling the model to learn from human feedback, rectify errors, and adapt to user preferences. This iterative learning approach enhances the model's overall performance, mitigating biases, improving accuracy, and enabling better alignment with user expectations. 

In this blog, we'll explore different ways to make Large Language Models (LLMs) better understand and respond to users' preferences. Firstly, we'll talk about the most talked method called Reinforcement Learning with Human Feedback (RLHF), using a reinforcement learning optimization called Proximal Policy Optimization (PPO). This helps the model learn from user feedback and improve its responses over time.

Next, we'll look at some modifications done by Meta for training their LLama-2 models. They use two reward models instead of one, making the model more sensitive to users' preferences. They also use rejection sampling to enable the model generate responses with maximum possible rewards. Lastly, we'll discuss a few alternatives to RLHF, like Direct Preference Optimization(DPO), and Reinforced Self-Training which mitigate the use of reward models and directly optimizing the policy(LLM) using the preference data.
