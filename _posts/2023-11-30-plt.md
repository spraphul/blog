---
title: Preference Training for LLMs in a Nutshell
---

![rlhf](https://github.com/spraphul/blog/blob/gh-pages/_posts/rlhf.png?raw=true)

Large Language Models (LLMs) harness the power of unsupervised learning(self-supervised to be precise) on extensive datasets, showcasing remarkable natural language processing capabilities. However, these models exhibit inherent limitations when not trained using Reinforcement Learning with Human Feedback (RLHF). One significant drawback lies in their lack of specificity and control. Although LLMs can generate contextually relevant outputs, they might fail to precisely adhere to user instructions, resulting in technically correct yet divergent responses. The absence of a mechanism for real-time adaptation to user feedback during inference further adds to this limitation, hindering the models from refining their outputs based on corrective input.

Ethical concerns and biases also affect LLMs not trained with RLHF. These models inadvertently showcase biases present in the training data, leading to outputs that may be skewed or unfair. Additionally, their vulnerability to generating incorrect or misleading information poses challenges in contexts where accuracy is critical. Despite their contextual understanding, LLMs may struggle with commonsense reasoning and language variations, impacting their ability to provide accurate and nuanced responses. Integrating RLHF into the training process addresses these flaws by enabling the model to learn from human feedback, rectify errors, and adapt to user preferences. This iterative learning approach enhances the model's overall performance, mitigating biases, improving accuracy, and enabling better alignment with user expectations. 

In this blog, we'll explore different ways to make Large Language Models (LLMs) better understand and respond to users' preferences. Firstly, we'll talk about the most talked method called Reinforcement Learning with Human Feedback (RLHF), using a reinforcement learning optimization called Proximal Policy Optimization (PPO). This helps the model learn from user feedback and improve its responses over time.

Next, we'll look at some modifications done by Meta for training their LLama-2 models. They use two reward models instead of one, making the model more sensitive to users' preferences. They also use rejection sampling to enable the model generate responses with maximum possible rewards. Lastly, we'll discuss a few alternatives to RLHF, like Direct Preference Optimization(DPO), and Reinforced Self-Training which mitigate the use of reward models and directly optimizing the policy(LLM) using the preference data.

## Reinforcement Learning with Human Feedback
RLHF was introduced by OpenAI in the paper titled "Training language models to follow instructions with human feedback". The authors describe their efforts in advancing the alignment of language models by training them to align with user intentions. They emphasize the importance of aligning language models with both explicit and implicit user intentions, such as following instructions, staying truthful, and avoiding biases or toxicity. The primary focus is on fine-tuning approaches, specifically using Reinforcement Learning from Human Feedback (RLHF) with Proximal Policy Optimization (PPO) to align the GPT-3 model to a broad range of written instructions.

The process involves hiring contractors to label data, collecting datasets of human-written demonstrations and comparisons, training reward models, and fine-tuning the model to maximize the reward using PPO. The resulting model, called InstructGPT, aligns with the preferences of a specific group of people, as opposed to a broader notion of "human values." Evaluation involves labeler ratings, automatic evaluations on NLP datasets, and comparisons between InstructGPT and GPT-3. Notable findings include labeler preferences for InstructGPT outputs, improvements in truthfulness, small gains in toxicity reduction, and generalization to preferences of held-out labelers. While exact training details for the best performing models like ChatGPT and GPT-4 haven't been released by OpenAI, the models' ability to adapt excellently to specific user preferences hints at RLHF as the core methodology behind the behaviour. 

A model having a behavior like ChatGPT is usually trained in three steps, unsupervised continual pretraining on a very large corpus followed by supervised finetuning on a high quality instruction-response pair dataset. At Last, RLHF comes into picture which is implemented as follows:

- Data Collection

Collect a dataset of human-labeled comparisons, where human annotators rank different model-generated outputs based on their quality or preference.

- Reward Model (RM) Training

Create a reward model by training it on the comparison data. The reward model predicts the preferences of human annotators by learning which model-generated outputs they prefer in different scenarios. The reward model in this case is also an LLM with the last token predictor replaced with a softmax layer which predicts a scalar reward for an instruction-response pair.

![reward](https://github.com/spraphul/blog/blob/gh-pages/_posts/reward.png?raw=true)

The reward model accepts an instruction(prompt) and two responses for it at a time. The loss function for the model is based on the objective of increasing the difference of rewards between the two pairs which can be shown as follows:

L<sub>reward</sub> = log(1 + e<sup>(R<sub>rejected</sub> - R<sub>chosen</sub>)</sup>)

Note: Training reward models in poses several challenges. One significant hurdle lies in obtaining high-quality and diverse human feedback data that accurately reflects the nuances of user preferences. Designing comparison datasets that effectively capture the subtle distinctions in model-generated outputs requires careful consideration, as human annotators might interpret and rank responses differently. Additionally, ensuring consistency and reliability among human labelers is challenging, as subjective judgments may vary. Managing the potential bias introduced by the preferences of a specific group of labelers and addressing the issue of conflicting feedback further complicates the reward model training process. Striking a balance between collecting sufficiently rich data for effective training and avoiding biases that may arise in the annotation process remains a persistent challenge in achieving robust and reliable reward models for RLHF.

- Policy(LLM) Update using PPO
The objective function for PPO is as follows:
![ppo](https://github.com/spraphul/blog/blob/gh-pages/_posts/PPO.png?raw=true)

where $πθ(a_t∣s_t)$ is the probability distribution for the current policy(LLM being updated) and $πθold(a_t∣s_t)$ is the probability distribution for the original(LLM at the start of the training) model and $A_t$ is the advantage function for the policy which is basically Reward<sub>current</sub> - Reward<sub>old</sub> in this context. The ppo objective function is clipped by a value ϵ so as to prevent the policy from getting updated catastrophically from the previous policy which can destabilize the training. 

Note that we are not adding the (cumulative future reward over time) term here because of the limited setting of RLHF as the instruction can keep varying across each data point and might not be in a sequential chat sequence mode. If we still want to incorporate the future rewards given that the training dataset is in a dialogue setting instead of single instruction-response setting, We can modify the advantage function as Reward<sub>current+future</sub> - Reward<sub>old</sub>, where the first term is a sum of rewards for model responses beginning from now until the conversation ends. 

Now that we have discussed the objective function, the policy(the model to be trained) is updated using an optimization algorithm such as gradient descent as follows:

θ<sub>new</sub> = θ<sub>old</sub> +α⋅∇<sub>θ</sub>L<sub>clip</sub>(θ)




