---
title : Visual Question Answering in Deep Learning
---

![VQA1](https://1.bp.blogspot.com/-MjwZXNHaiNw/XawRlU4ccyI/AAAAAAAAPhE/8a70_NsgDT0xN5A7aB6MYfZp5N3kR9pYQCLcBGAsYHQ/s1600/vqa_logo_70.png)

As research in machine learning has advanced so far and so has this blog ðŸ˜ƒ, we will be discussing the topic of visual
question answering in this blog. As the name suggests, we will be given an image as the knowledge domain and a text-based 
question will be asked. Based on all these we will have to answer that question.

**We will be building our model from scratch and also discuss upon improvising the same.

The model structure will be as follows:**

![VQA2](https://1.bp.blogspot.com/-mkqnb_X97wU/Xa37DHJjEuI/AAAAAAAAPjw/PXao2ta9r90SObfjqv69gyXV7XahqyXpACLcBGAsYHQ/s1600/Untitled%2BDiagram%2B%25281%2529.png)

### DATASET:
There are a lot of datasets we can work upon but currently, our main focus is to be able to build a deep learning model for 
answering the queries based on an image.

As of now, we will make an assumption about the dataset that it contains a pre-processed image of size **50x50** pixels and the 
question has a **maximum length=10 words** and has an answer of **maximum length=5 words.**


**Let us define the various parameters first:**

<script src="https://gist.github.com/spraphul/71c2b6eb6a6184113c85bb024f7aa82f.js"></script>

Here **VOCAB_ANS_SIZE** is a separate vocab which is smaller in size as compared to the actual vocab so as to reduce the 
computation. We should build a separate vocab which consists of words from the answers in the training data only.


**Now, let us build the Image encoder:**

<script src="https://gist.github.com/spraphul/dcb28162e6d2db5460da351d267017ba.js"></script>

We will use **repeat vector** for the encoded image vector so that we embed the image information to each of the words of the 
question by concatenating the image vector to each word of the question sequence.



**Let us now build the encoder for the question+image:**

<script src="https://gist.github.com/spraphul/0aa84b5c86c51d99172d6746735339c2.js"></script>

We will feed the last state of the encoder as the initial_state in the decoder layer. Below is the code for decoder layer 
which finally computes the probability of **5 words** in the answer from the **VOCAB_ANS.**

<script src="https://gist.github.com/spraphul/fb253c1f76393da77e7486f46e4a2bd2.js"></script>

**Now we are ready with the architecture and we can define our final model and loss functions as follows:**

<script src="https://gist.github.com/spraphul/9740fa702d3aa12f26051e73b6c16a40.js"></script>

Since we are using **sparse categorical cross_entropy** function as our loss function, we don't need to send the output as 
one-hot vectors.

**Note:** The confusing point, decoder inputs are nothing but a sequence of **UNKNOWN_TOKENS**  of length=5words. This is to 
initialise the decoder layer. We need to pass that separately to the model while training and prediction.

**Finally, we are ready with a visual question answering model building it from scratch. We can improve the image encoding by 
using Transfer-learning and passing the image to a pre-trained model like Inception, VGG, Resnet etc to obtain better image feature.**

Hope you like reading this blog. 

**Keep Learning, Keep Sharing ðŸ˜Š.**
