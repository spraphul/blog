---
title : Incremental Learning Without Forgetting
---

One of the major area of concern in deep learning is the generalisation problem. This has been a hot topic for research for the past few years. Generally what happens is that we get a use case, we build a model for that and we push it in production. But what if we have a slight change in our problem statement. Do we need to solve it once again from the scratch? What if we dont have the dataset we had previously? We seek for a way to preserve the previous learning of the system and work on just the evolution part. And we will be talking about one of the similar aspects of the problem in this blog. So, welcome to the new blog of **Learn & Share** thread and bear with me for some of the next few paragraphs.


### Incremental Learning
Let us define the problem statement first and then we shall discuss the various solutions for that. Suppose you have a dataset for 5 classes and you built a deep learning network for the classification problem. Now, let us imagine you have the model but you lost the dataset and you need to add an extra class for the existing problem statement. Well, let us not get to the solution that quickly. Instead, let us derive the solution from the history itself.

![il1](https://d3i71xaburhd42.cloudfront.net/ce387a6ac00c1e23a9e1aa4a2ce4800b1066e177/2-Figure1-1.png)


One of the famous start in the approaches of generalisation is **Transfer Learning** which has infact proved to be very successful. So, in this approach we already have a pre-trained model for a dataset1. Now, if a new dataset2, which comes from a similar kind of domain, we can use the knowledge of the previous pre-trained model instead of training it again from scratch. What happens is that we initialise the new model with the same weights as of the pre-trained model and add a new softmax layer removing the last layer of the previous model(cosidering it is a classification problem). And we train the model, starting from the point where the pre-trained model had reached already. This results in fast and better convergence. If the previous model is very large, we do not have to update the weights for its previous layers(you can decide how many) by freezing them(making them untrainable). We update only the weights of the extra layers(some previous layers too if you want) we have added. This strategy is called **Fine Tuning**.

![sjk](https://miro.medium.com/max/1838/1*9GTEzcO8KxxrfutmtsPs3Q.png)
