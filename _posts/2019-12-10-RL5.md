---
title : Reinforcement Learning &#58; Proximal Policy Optimization(PPO)
---

In this blog, we will be digging into another reinforcement learning algorithm by OpenAI, **Trust Region Policy Optimization** followed by **Proximal Policy Optimization**. Before discussing the algorithm directly, let us understand some of the concepts and reasonings for better explanations.

**ON & OFF Policies** : In one of the previous blogs of the reinforcement learning thread, we studied about deep Q-learning where we used to keep a replay buffer memory to store the previous states and randomly chose a batch to train the model. This type of strategy is said to be **OFF** as they do not update the model based on the current performance. The **ON** policy models, on the other hand, update the model episode by episode based on the current exploration of the agent. Like A2C and A3C, TRPO and PPO also are **ON-Policy** algorithms. ON Policy algorithms are generally slow to converge and a bit noisy because they use an exploration only once. 


### Trust Region Policy Optimization
Updating the weights of a neural network repeatedly for a batch pushes the policy function far away from its initial estimation in Q-learning and this is the issue which the TRPO takes very seriously. So, the idea is to update the policy function but not allowing it to change much from the previous policy by introducing a constraint for it.


#### Gradients
Let us recall the gradient policy we used in the previous blog for A2C and A3C algorithms:

![kdl](https://1.bp.blogspot.com/-j0ublUid7NQ/XeQDR333PCI/AAAAAAAAQFg/Y8g0syT7d4EN4fEajGS6D6iLUlTHY_vfwCLcBGAsYHQ/s1600/policy_gradient.pn)

The logarithm of the probability is now replaced with a ratio of the probabilty by the current policy to that of the probabilty of the old policy and the loss function looks like the one shown below:

![qswkd](https://miro.medium.com/max/1476/0*S949lemw0fEDVPJE)

The ratio, **r<sub>t</sub>** has a value greater than one if the current policy is more likey to happen than the previous one and has a value between 0 and 1 if it is less likely than the previous one. 

For the updated policy to lie in the trust region, an extra constraint is added in the form of KL divergence as shown below:

![dsn](https://miro.medium.com/max/2350/1*IaBgY-p9fgwupuaB_jUJaA@2x.jpeg)

Using Lagrange's multipliers for constrained optimizations, the final optimization problem looks like this :

![lsak](https://2.bp.blogspot.com/-Vy2_aAl-iqs/Xfn_00NK_uI/AAAAAAAAQNk/R_cSS92W2ZgSKOQTh-g7-Dbd9W6a7WLYQCLcBGAsYHQ/s1600/Screenshot%2B2019-12-18%2Bat%2B3.56.09%2BPM.png)

where Î² is a constant hyper-parameter.

Too much of computations, right?

We have come to a point now where we can discuss our next algorithm.

### Proximal Policy Optimization
This is a modified version of the TRPO where we can now have a single policy taking care of both the updation logic and the trust region. PPO comes up with a clipping mechanism which clips the **r<sub>t</sub>** between a given range and does not allow it to go further away from the range. 

![dejs](https://i.stack.imgur.com/zt9mz.png)

**So what is this clipping thing?** 

Let me describe this you by an example :

Suppose we have a vector, v = [0.2, 0.6, 0.4, 0.1] and let us clip this between a range, [0.3, 0.5].

Upon clipping, the vector v = [0.3, 0.5, 0.4, 0.3], meaning the values which are less than the minimum value of the range, will be assigned the minimum value of the range and the value which are greater than the maximum value of the range will be assigned the maximum value of the range. 


Now, let us understand this function by looking at its graph given below:

![hmjs](https://media.arxiv-vanity.com/render-output/1627307/f9.jpg)


