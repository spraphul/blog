---
title : Reinforcement Learning &#58; Proximal Policy Optimization(PPO)
---

In this blog, we will be digging into another reinforcement learning algorithm by OpenAI, **Trust Region Policy Optimization** followed by **Proximal Policy Optimization**. Before discussing the algorithm directly, let us understand some of the concepts and reasonings for better explanations.

**ON & OFF Policies** : In one of the previous blogs of the reinforcement learning thread, we studied about deep Q-learning where we used to keep a replay buffer memory to store the previous states and randomly chose a batch to train the model. This type of strategy is said to be **OFF** as they do not update the model based on the current performance. The **ON** policy models, on the other hand, update the model episode by episode based on the current exploration of the agent. Like A2C and A3C, TRPO and PPO also are **ON-Policy** algorithms. ON Policy algorithms are generally slow to converge and a bit noisy because they use an exploration only once. 


### Trust Region Policy Optimization
Updating the weights of a neural network repeatedly for a batch pushes the policy function far away from its initial estimation in Q-learning and this is the issue which the TRPO takes very seriously. So, the idea is to update the policy function but not allowing it to change much from the previous policy by introducing a constraint for it.


#### Gradients
Let us recall the gradient policy we used in the previous blog for A2C and A3C algorithms:

![kdl](https://1.bp.blogspot.com/-j0ublUid7NQ/XeQDR333PCI/AAAAAAAAQFg/Y8g0syT7d4EN4fEajGS6D6iLUlTHY_vfwCLcBGAsYHQ/s1600/policy_gradient.pn)

The logarithm of the probability is now replaced with a ratio of the probabilty by the current policy to that of the probabilty of the old policy and the loss function looks like the one shown below:

![qswkd](https://miro.medium.com/max/1476/0*S949lemw0fEDVPJE)

The ratio, **r<sub>t</sub>** has a value greater than one if the current policy is more likey to happen than the previous one and has a value between 0 and 1 if it is less likely than the previous one. 

For the updated policy to lie in the trust region, an extra constraint is added in the form of KL divergence as shown below:

![dsn](https://miro.medium.com/max/2350/1*IaBgY-p9fgwupuaB_jUJaA@2x.jpeg)

Using Lagrange's multipliers for constrained optimizations, the final optimization problem looks like this :

![lsak](https://miro.medium.com/max/3200/1*UTNJbhabWk7RSqfvRQ1KDg.jpeg)

