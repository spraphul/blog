---
title : Reinforcement Learning &#58; Actor-Critic Networks
---

In the previous blog, we dived into the basic implementation of a deep Q-Learning Neural Network. It was a Policy-based duel- network which was used to learn the thief-police-gold game. Now, I have all of a sudden introduced two terms here, **Policy-Based, Duel-Network**. Policy-based methods are those which learns the probability distribution of the actions to take next of  being in a given state. As, it could be seen that we were using a softmax layer of output-size=**number of possible actions**, it was nothing but the policy-learning mechanism for the network to learn which action to take further depending on the probabilities. Moving on to the next term, **Duel-Network**. We had used two neural-networks in the previous blog, one which was used and updated in an online manner while the other was updated(not so often) and used for predicting the policy values for the new_state. The reason for that was to be able to maintain some sort of consistency and not induce much randomness in the poilcy-prediction for the new states which are used to update the current policy-values. 

![a3c1](https://www.mdpi.com/sensors/sensors-19-01547/article_deploy/html/images/sensors-19-01547-g002.png)


Coming to this blog, I will be talking about a new term again ðŸ˜€ and that is **Actor-Critic** Methods. The previous agent can be understood as the actor part of the actor-critic networks as it focuses on the policy-prediction. To be able to understand the critic part, we need to understand value-based networks. So, unlike policy based networks critic-network predicts the value of importance of being in a state. In fact, reinforcement learning started with value-based networks and the policy-based learning was derived using the equation of value-equation. Let us go into some maths this time ðŸ˜Š:


![a3c2](https://miro.medium.com/max/2640/1*c2bAqQu5jcXy7JfmJZLxpw.png)




